# -*- coding: utf-8 -*-
"""Book Recommendation System in Popularity Based & Collaborate Filtering Based.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kjGVQw5NdHebxYn8Sex-TAPrKAiYJ-h2
"""

##### Import all necessity functions for Machine Learning #####
import sys
import math
import string
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy as shc
import warnings
import zipfile
import cv2
import os
import random
from collections import Counter
from functools import reduce
from itertools import chain
from google.colab.patches import cv2_imshow
from keras.preprocessing import image
from sklearn.metrics._plot.confusion_matrix import confusion_matrix
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression, SelectKBest, chi2, VarianceThreshold
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTEN, SMOTENC, SVMSMOTE, KMeansSMOTE, BorderlineSMOTE, ADASYN
from imblearn.ensemble import EasyEnsembleClassifier
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor, NearestNeighbors
from sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier, SGDRegressor, Perceptron
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, ExtraTreeClassifier, ExtraTreeRegressor
from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor, VotingClassifier, VotingRegressor
from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, GradientBoostingClassifier, GradientBoostingRegressor, StackingClassifier, StackingRegressor
from sklearn.metrics import classification_report, mean_absolute_error, mean_squared_error, r2_score, accuracy_score, recall_score, precision_score, f1_score, silhouette_score
from xgboost import XGBClassifier, XGBRegressor

##### Download keras #####
!pip install keras

##### Import all necessity functions for Neural Network #####
import tensorflow as tf
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.utils import plot_model
from tensorflow.keras.layers import Dense, Conv2D, LSTM, GRU, RNN, Flatten, AvgPool2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization, Dropout, LeakyReLU, ELU, PReLU
from tensorflow.keras.activations import tanh, relu, sigmoid, softmax, swish
from tensorflow.keras.regularizers import L1, L2, L1L2
from tensorflow.keras.optimizers import SGD, Adagrad, Adadelta, RMSprop, Adam, Adamax, Nadam
from tensorflow.keras.initializers import HeNormal, HeUniform, GlorotNormal, GlorotUniform
from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy, hinge, MSE, MAE, Huber
import keras.utils as image

##### Remove all warnings #####
import warnings
warnings.filterwarnings("ignore")


##### Using, NearestNeighbours, we will find the closest point of that particular input #####
from sklearn.neighbors import NearestNeighbors

##### Import all the datasets #####
try:
  book_df   = pd.read_csv('/content/Books.csv')
  rating_df = pd.read_csv('/content/Ratings.csv')
  users_df  = pd.read_csv('/content/Users.csv')
except Exception as e:
  print(e.with_traceback)
else:
  print('Datasets imported.'.capitalize())

##### Show the book dataset #####
book_df.head()

##### Show the ratings dataset #####
rating_df.head()

##### Show the users df #####
users_df.head()

##### Show the all features of all datasets #####
def show_features_name(book_df, rating_df, users_df):
  return book_df.columns, rating_df.columns, users_df.columns

if __name__ == "__main__":
  try:
    book_columns, rating_columns, users_columns = show_features_name(book_df, rating_df, users_df)
  except Exception as e:
    print(e.with_traceback)
  else:
    print('The features list of book dataset is   {}'.format(book_columns),'\n')
    print('*'*120)
    print('The features list of rating dataset is {}'.format(rating_columns),'\n')
    print('*'*120)
    print('The features list of users dataset is  {}'.format(users_columns))

"""
***User-ID is common in Ratings and Users Dataset.***
***ISBN is common in Books and Ratings Dataset.***
"""

##### Show the shape of all the datsets #####
def show_shape(book_df, rating_df, users_df):
  return book_df.shape, rating_df.shape, users_df.shape

if __name__ == "__main__":
  try:
    book_shape, rating_shape, user_shape = show_shape(book_df, rating_df, users_df)
  except Exception as e:
    print(e.with_traceback)
  else:
    print('The shape of book is   {}'.format(book_shape),'\n')
    print('The shape of rating is {}'.format(rating_shape),'\n')
    print('The shape of user is   {}'.format(user_shape),'\n')

print('The number of length of User-ID in ratings dataset is = {}'.format(len(rating_df.loc[:, 'User-ID'])),'\n')
print('The number of unique length of User-Id in rating datset is = {}'.format(rating_df.loc[:, 'User-ID'].nunique()),'\n')
print('The number of duplicated length of User-Id in rating datset is = {}'.format(rating_df.loc[:, 'User-ID'].duplicated().sum()))

# """
# **** Meaning That, in Ratings dataset - ONE user gives multiple ratings on IBSN books.****
# """

##### Find out top 10 User -ID who rates the books ####
plt.figure(figsize = (6, 4))
plt.title('Find the top - 10 User -ID.')
rating_df.loc[:, 'User-ID'].value_counts().sort_values(ascending = False).head(10).plot(kind = 'barh')
plt.show()

##### Find out the top 5 books based on top 5 - users #####
def find_highest_rating(group):
  return group[group['Book-Rating'] == group['Book-Rating'].max()]

rating_df[rating_df.loc[:, 'User-ID'].isin(rating_df.groupby(['User-ID'])['User-ID'].\
                                           count().sort_values(ascending = False).head(10).index)].\
                                           groupby(['User-ID']).apply(find_highest_rating).\
                                           drop_duplicates(subset = ['User-ID'])

##### Find out the age of top - 10 best users #####
plt.title('The age of top 10 users based on their counting book rate')
users_df[users_df.loc[:, 'User-ID'].isin(rating_df.groupby(['User-ID']).\
                                count()['Book-Rating'].sort_values(ascending = False).\
                                head(10).index)]['Age'].plot(kind = 'bar')

plt.show()

"""Popularity Based Recommendation System"""

# """
# In this technique, we will consider those book which avgerage rating is higher AND those book will be taken which obtained more than 250 votes.
# """

df = book_df.merge(rating_df, on = 'ISBN')
new_df = df.loc[:, ['ISBN', 'Book-Title', 'Book-Author', 'User-ID', 'Book-Rating']]
new_df.head()

##### Find the average rating #####
avg_rating = new_df.groupby(['Book-Title']).mean()['Book-Rating'].\
                              reset_index().rename(columns = \
                              {'Book-Rating': 'avg_rating'})
avg_rating.head()

##### Find the count rating #####
count_rating = new_df.groupby(['Book-Title']).count()['Book-Rating'].reset_index().rename(columns = {'Book-Rating': 'count_rating'})

count_rating.head()

##### Concatenation of the dataset #####
new_DF = pd.concat([avg_rating, count_rating], axis = 1)
##### Find those 50 books there voting is more 200 and avg_rating is higher #####
new_DF[new_DF.loc[:, 'count_rating'] > 200].sort_values(by = 'avg_rating', ascending = False).head(50)

def recommendation():
  df = book_df.merge(rating_df, on = 'ISBN')
  new_df = df.loc[:, ['ISBN', 'Book-Title', 'Book-Author', 'User-ID', 'Book-Rating']]
  ##### Find the average rating #####
  avg_rating = new_df.groupby(['Book-Title']).mean()['Book-Rating'].\
                                reset_index().rename(columns = \
                                {'Book-Rating': 'avg_rating'})
  ##### Find the count rating #####
  count_rating = new_df.groupby(['Book-Title']).count()['Book-Rating'].\
                                reset_index().rename(columns = \
                                {'Book-Rating': 'count_rating'})
  ##### Concatenation of the dataset #####
  new_DF = pd.concat([avg_rating, count_rating], axis = 1)
  ##### Find those 50 books there voting is more 200 and avg_rating is higher #####
  return new_DF[new_DF.loc[:, 'count_rating'] > 200].sort_values(by = 'avg_rating', ascending = False).head(50)

if __name__ == "__main__":
   book_name = recommendation()

##### Top 50 recommendated book #####
book_name['Book-Title'].iloc[:, 0].values

"""Collaborate Filtering based Recommendation"""

##### Merge the rating dataframe and book dataframe ####
df = book_df.merge(rating_df, on = 'ISBN')
new_df = df.loc[:, ['ISBN', 'Book-Title', 'Book-Author', 'User-ID', 'Book-Rating']]
new_df

##### Take those users who rate more tha 200 ratings #####
check  = new_df.groupby('User-ID').count()['Book-Rating'] > 200
check  = check[check].index 
new_df = new_df[new_df.loc[:, 'User-ID'].isin(check)]
new_df.head()

##### Take those books which count of rating more the 50 #####
check = new_df.groupby(['Book-Title']).count()['Book-Rating'] >= 50
check = check[check].index 
new_df = new_df[new_df.loc[:, 'Book-Title'].isin(check)]
new_df.head()

##### Draw the pivot_table #####
new_df.drop_duplicates(inplace = True)
dataset = new_df.pivot_table(index = 'Book-Title', columns = 'User-ID', values = 'Book-Rating', fill_value = 0)
dataset.head()

from sklearn.metrics.pairwise import cosine_similarity

cosine_similarity_ = cosine_similarity(dataset)
print('The cosine similarity is {}'.format(cosine_similarity_))

##### Create the recommendation #####
def recommendation(book):
  index = sorted(list(enumerate(cosine_similarity_[np.where(dataset.index == book)[0][0]])),\
         key = lambda x: x[1], reverse = True)[1:6]

  for index_ in index:
    print(dataset.index[index_[0]])

if __name__ == "__main__":
  recommendation('Harry Potter and the Chamber of Secrets (Book 2)')

def find(group):
  return group[group['Book-Rating'] > 6]
a = rating_df.groupby(['User-ID']).apply(find)

a['User-ID']

b = a.reset_index(level = [1]).drop(['User-ID'], axis = 1).reset_index()
b

c = b.groupby(['User-ID']).count()
c

c['Book-Rating'].sort_values(ascending = False).head(10000).index

d = book_df.merge(rating_df, on = 'ISBN')
d

for book in d[d['User-ID'].isin(c['Book-Rating'].sort_values(ascending = False).\
                    head(500).index)]['Book-Title'].\
                    value_counts(ascending = False).head(50).index:
    print(book, end = "\n*-*")

merged_data = book_df.merge(rating_df, on = 'ISBN')
merged_data.drop_duplicates(inplace = True)
merged_data

'''
1. Take those rating which are above 7 
2. Then take top 10000 users who rated most
3. Find the User-ID from there
4. Lastly fing top 50 books
'''

merged_data = merged_data[merged_data['Book-Rating'] > 7]
merged_data

merged_data.groupby(['User-ID']).count()

c = merged_data[merged_data.loc[:, 'User-ID'].isin(merged_data.groupby(['User-ID']).count()['Book-Rating'].sort_values(ascending = False).head(10000).index)]
c

c.index

c.index.nunique()

c.groupby(['Book-Title']).count()['Book-Rating'].sort_values(ascending = False).head(50)